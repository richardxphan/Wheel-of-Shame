{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4710da26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Remove the placeholder and ensure pandas is imported only once\n",
    "# Since pandas is already imported in other cells, you can remove the placeholder entirely\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the dataset\n",
    "data_folder = os.path.join(os.getcwd(), './data')\n",
    "dataset_path = os.path.join(data_folder, 'Collisions Dataset.csv')\n",
    "\n",
    "# Read the dataset\n",
    "try:\n",
    "    collisions_df = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file 'collisions.csv' was not found in the 'data' folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3f793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date & Time' column to datetime format\n",
    "collisions_df[\"Date & Time\"] = pd.to_datetime(collisions_df[\"Date & Time\"], format='%m/%d/%Y %I:%M %p')\n",
    "\n",
    "# Extract the hour from the time portion\n",
    "collisions_df[\"Hour\"] = collisions_df[\"Date & Time\"].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2b232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to categorize the time of day\n",
    "def categorize_time_of_day(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return \"Morning\"\n",
    "    elif 12 <= hour < 18:\n",
    "        return \"Afternoon\"\n",
    "    elif 18 <= hour < 24:\n",
    "        return \"Evening\"\n",
    "    elif 0 <= hour < 6:\n",
    "        return \"Night\"\n",
    "    else:\n",
    "        return \"NaN\"\n",
    "\n",
    "# Apply the function to categorize based on the hour\n",
    "collisions_df[\"Time of Day\"] = collisions_df[\"Hour\"].apply(categorize_time_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b500eaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataframe with time of day:\n",
      "                Date & Time  Hour Time of Day\n",
      "0       2014-06-16 05:28:00     5       Night\n",
      "1       2015-07-09 13:18:00    13   Afternoon\n",
      "2       2014-08-22 18:22:00    18     Evening\n",
      "3       2014-01-01 01:09:00     1       Night\n",
      "4       2014-01-01 01:47:00     1       Night\n",
      "...                     ...   ...         ...\n",
      "3804262 2023-06-02 23:54:00    23     Evening\n",
      "3804263 2023-06-07 23:43:00    23     Evening\n",
      "3804264 2023-02-21 09:19:00     9     Morning\n",
      "3804265 2023-08-11 19:56:00    19     Evening\n",
      "3804266 2023-07-01 17:37:00    17   Afternoon\n",
      "\n",
      "[3804267 rows x 3 columns]\n",
      "Counts of each time of day category:\n",
      "Time of Day\n",
      "Afternoon    1640235\n",
      "Morning       981954\n",
      "Evening       904177\n",
      "Night         277901\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"New dataframe with time of day:\")\n",
    "print(collisions_df[[\"Date & Time\", \"Hour\", \"Time of Day\"]])\n",
    "time_of_day = collisions_df[\"Time of Day\"].value_counts()\n",
    "print(\"Counts of each time of day category:\")\n",
    "print(time_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfeb84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of each driver age category:\n",
      "Driver Age (Crash Level) \n",
      "0                   41602\n",
      "[0,0]               38761\n",
      "19                  23918\n",
      "21                  23742\n",
      "22                  23608\n",
      "                    ...  \n",
      "[25,27,38,59]           1\n",
      "[46,56,94]              1\n",
      "[0,52,55,64]            1\n",
      "[28,54,54,58,65]        1\n",
      "[22,46,66,69]           1\n",
      "Name: count, Length: 82494, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "age_df = collisions_df[\"Driver Age (Crash Level) \"].value_counts()\n",
    "print(\"Counts of each driver age category:\")\n",
    "print(age_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce4ec292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (3804267, 31)\n",
      "   Collision ID  Posted Speed (Crash Level)   Speed Limit  \\\n",
      "0       4512922                          0.0          NaN   \n",
      "1       4527603                          0.0          NaN   \n",
      "2       4547444                         35.0          NaN   \n",
      "3       4691880                         45.0         35.0   \n",
      "4       4691882                         25.0         30.0   \n",
      "\n",
      "              Speed Related  Distracted Driver (Suspected)  \\\n",
      "0  [false,false,false,true]                          False   \n",
      "1              [false,true]                          False   \n",
      "2       [false,false,false]                          False   \n",
      "3                     false                           True   \n",
      "4             [false,false]                          False   \n",
      "\n",
      "  Distracted Driver Related (Confirmed) Suspected At Fault  \\\n",
      "0             [false,false,false,false]                Yes   \n",
      "1                         [false,false]                Yes   \n",
      "2                   [false,false,false]      [\"Yes\",\"Yes\"]   \n",
      "3                                 false                NaN   \n",
      "4                         [false,false]                NaN   \n",
      "\n",
      "  Aggressive Driving Related  Red Light Running T/F  Severity Score  ...  \\\n",
      "0   [false,false,false,true]                  False             1.0  ...   \n",
      "1               [false,true]                  False             1.0  ...   \n",
      "2        [false,false,false]                  False             1.0  ...   \n",
      "3                      false                  False             1.0  ...   \n",
      "4              [false,false]                  False             1.0  ...   \n",
      "\n",
      "   Person ID                       Roadway Contributing Factors  \\\n",
      "0          1                            No Contributing Factors   \n",
      "1          1                            No Contributing Factors   \n",
      "2  [1,3,4,6]  [\"No Contributing Factors\",\"No Contributing Fa...   \n",
      "3          1                            No Contributing Factors   \n",
      "4          1  [\"No Contributing Factors\",\"No Contributing Fa...   \n",
      "\n",
      "   Surface Condition (Crash Level)  Weather Conditions (Crash Level)  \\\n",
      "0                               Dry                            Clear   \n",
      "1                               Dry                            Clear   \n",
      "2                               Dry                            Clear   \n",
      "3                               Dry                            Clear   \n",
      "4                               Dry                            Clear   \n",
      "\n",
      "  Light Conditions (Crash Level)  \\\n",
      "0               Dark-Not Lighted   \n",
      "1                       Daylight   \n",
      "2                       Daylight   \n",
      "3                   Dark-Lighted   \n",
      "4                   Dark-Lighted   \n",
      "\n",
      "                                     Traffic Control Work Zone Indicator  \\\n",
      "0                                          Stop Sign                 NaN   \n",
      "1                                              Lanes                 NaN   \n",
      "2  [\"No Control Present\",\"No Control Present\",\"No...                 NaN   \n",
      "3                                              Lanes                 NaN   \n",
      "4        [\"No Control Present\",\"No Control Present\"]                 NaN   \n",
      "\n",
      "   # of Injuries per crash   KABCO Severity  \\\n",
      "0                         0   (O) No Injury   \n",
      "1                         0   (O) No Injury   \n",
      "2                         0   (O) No Injury   \n",
      "3                         0   (O) No Injury   \n",
      "4                         0   (O) No Injury   \n",
      "\n",
      "           Vehicle Contributing Factor (Crash Level)  \n",
      "0                                                NaN  \n",
      "1                                   No Known Defects  \n",
      "2  [\"No Known Defects\",\"No Known Defects\",\"Slick ...  \n",
      "3                                   No Known Defects  \n",
      "4            [\"No Known Defects\",\"No Known Defects\"]  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "dtype_types = {\n",
    "    'Collision ID': 'float32',\n",
    "    'Posted Speed (Crash Level)': 'float32',  # Assuming numeric; may adjust if needed\n",
    "    'Speed Limit': 'float32',\n",
    "    'Speed Related': 'category',              # Likely 'Y'/'N' or boolean\n",
    "    'Distracted Driver (Suspected)': 'category',\n",
    "    'Distracted Driver Related (Confirmed)': 'category',\n",
    "    'Suspected At Fault': 'category',\n",
    "    'Aggressive Driving Related': 'category',\n",
    "    'Red Light Running T/F': 'category',      # 'T'/'F'\n",
    "    'Severity Score': 'float32',\n",
    "    '# of Fatalities per Crash': 'int16',\n",
    "    '# of Injuries per crash': 'int16',\n",
    "    'Date & Time': 'object',                 # load as string initially (will parse later if needed)\n",
    "    'Driver Condition (Crash Level)': 'category',\n",
    "    'Driver Age (Crash Level)': 'float32',\n",
    "    'Hit & Run Related': 'category',\n",
    "    'Impaired Driving (Confirmed)': 'category',\n",
    "    'Lat': 'float32',\n",
    "    'Long': 'float32',\n",
    "    'Person ID': 'int32',\n",
    "    'Roadway Contributing Factors': 'category',\n",
    "    'Surface Condition (Crash Level)': 'category',\n",
    "    'Weather Conditions (Crash Level)': 'category',\n",
    "    'Light Conditions (Crash Level)': 'category',\n",
    "    'Traffic Control': 'category',\n",
    "    'KABCO Severity': 'category',\n",
    "    'Vehicle Contributing Factor (Crash Level)': 'category'\n",
    "}\n",
    "\n",
    "def fill_na_int(val):\n",
    "    if val in ('', 'NA', 'NaN'):\n",
    "        return -1  # some sentinel\n",
    "    return int(val)\n",
    "\n",
    "df = pd.read_csv(dataset_path, converters={'Collision ID': fill_na_int}, parse_dates=['Date & Time'], \n",
    "                 low_memory=False)\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9753dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (3804267, 31)\n",
      "   Collision ID  Posted Speed (Crash Level)   Speed Limit  \\\n",
      "0       4512922                          0.0          NaN   \n",
      "1       4527603                          0.0          NaN   \n",
      "2       4547444                         35.0          NaN   \n",
      "3       4691880                         45.0         35.0   \n",
      "4       4691882                         25.0         30.0   \n",
      "\n",
      "              Speed Related  Distracted Driver (Suspected)  \\\n",
      "0  [false,false,false,true]                          False   \n",
      "1              [false,true]                          False   \n",
      "2       [false,false,false]                          False   \n",
      "3                     false                           True   \n",
      "4             [false,false]                          False   \n",
      "\n",
      "  Distracted Driver Related (Confirmed) Suspected At Fault  \\\n",
      "0             [false,false,false,false]                Yes   \n",
      "1                         [false,false]                Yes   \n",
      "2                   [false,false,false]      [\"Yes\",\"Yes\"]   \n",
      "3                                 false                NaN   \n",
      "4                         [false,false]                NaN   \n",
      "\n",
      "  Aggressive Driving Related  Red Light Running T/F  Severity Score  ...  \\\n",
      "0   [false,false,false,true]                  False             1.0  ...   \n",
      "1               [false,true]                  False             1.0  ...   \n",
      "2        [false,false,false]                  False             1.0  ...   \n",
      "3                      false                  False             1.0  ...   \n",
      "4              [false,false]                  False             1.0  ...   \n",
      "\n",
      "   Person ID                       Roadway Contributing Factors  \\\n",
      "0          1                            No Contributing Factors   \n",
      "1          1                            No Contributing Factors   \n",
      "2  [1,3,4,6]  [\"No Contributing Factors\",\"No Contributing Fa...   \n",
      "3          1                            No Contributing Factors   \n",
      "4          1  [\"No Contributing Factors\",\"No Contributing Fa...   \n",
      "\n",
      "   Surface Condition (Crash Level)  Weather Conditions (Crash Level)  \\\n",
      "0                               Dry                            Clear   \n",
      "1                               Dry                            Clear   \n",
      "2                               Dry                            Clear   \n",
      "3                               Dry                            Clear   \n",
      "4                               Dry                            Clear   \n",
      "\n",
      "  Light Conditions (Crash Level)  \\\n",
      "0               Dark-Not Lighted   \n",
      "1                       Daylight   \n",
      "2                       Daylight   \n",
      "3                   Dark-Lighted   \n",
      "4                   Dark-Lighted   \n",
      "\n",
      "                                     Traffic Control Work Zone Indicator  \\\n",
      "0                                          Stop Sign                 NaN   \n",
      "1                                              Lanes                 NaN   \n",
      "2  [\"No Control Present\",\"No Control Present\",\"No...                 NaN   \n",
      "3                                              Lanes                 NaN   \n",
      "4        [\"No Control Present\",\"No Control Present\"]                 NaN   \n",
      "\n",
      "   # of Injuries per crash   KABCO Severity  \\\n",
      "0                         0   (O) No Injury   \n",
      "1                         0   (O) No Injury   \n",
      "2                         0   (O) No Injury   \n",
      "3                         0   (O) No Injury   \n",
      "4                         0   (O) No Injury   \n",
      "\n",
      "           Vehicle Contributing Factor (Crash Level)  \n",
      "0                                                NaN  \n",
      "1                                   No Known Defects  \n",
      "2  [\"No Known Defects\",\"No Known Defects\",\"Slick ...  \n",
      "3                                   No Known Defects  \n",
      "4            [\"No Known Defects\",\"No Known Defects\"]  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "def fill_na_int(val):\n",
    "    if val in ('', 'NA', 'NaN'):\n",
    "        return -1  # sentinel\n",
    "    return int(val)\n",
    "\n",
    "# CSV path (replace with your actual file path)\n",
    "dataset_path = \"./data/Collisions Dataset.csv\"\n",
    "\n",
    "# 1) Load data, using a converter for 'Collision ID', parse 'Date & Time'\n",
    "df = pd.read_csv(\n",
    "    dataset_path, \n",
    "    converters={'Collision ID': fill_na_int},  # fill NA with -1\n",
    "    parse_dates=['Date & Time'], \n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.head(5))\n",
    "\n",
    "# 2) If you want to treat Collision ID with missing (-1) as a real 'missing' integer, you can convert it:\n",
    "df['Collision ID'] = df['Collision ID'].replace(-1, pd.NA)    # turn sentinel -1 into real missing\n",
    "df['Collision ID'] = df['Collision ID'].astype('Int32')       # Pandas extension type that supports NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e9d3439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial load - shape: (3804267, 31)\n",
      "[Label Encoding] 'Suspected At Fault.1' has 158 unique categories.\n",
      "[Label Encoding] 'Driver Condition (Crash Level) ' has 312 unique categories.\n",
      "[Label Encoding] 'Driver Age (Crash Level) ' has 82495 unique categories.\n",
      "[Label Encoding] 'Person ID' has 1715 unique categories.\n",
      "[Label Encoding] 'Roadway Contributing Factors' has 574 unique categories.\n",
      "[Label Encoding] 'Traffic Control' has 824 unique categories.\n",
      "[Label Encoding] 'Vehicle Contributing Factor (Crash Level)' has 336 unique categories.\n",
      "One-hot encoding these columns: ['Drug Test Results (Crash Level) ', 'Surface Condition (Crash Level) ', 'Weather Conditions (Crash Level)', 'Light Conditions (Crash Level)', 'KABCO Severity']\n",
      "Shape after encoding: (3804267, 70)\n",
      "Preprocessing complete. Final shape: (3804267, 71)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def fill_na_int(val):\n",
    "    if val in ('', 'NA', 'NaN'):\n",
    "        return -1\n",
    "    return int(val)\n",
    "\n",
    "# 1) Load data with converter for Collision ID\n",
    "df = pd.read_csv(\n",
    "    dataset_path,\n",
    "    converters={'Collision ID': fill_na_int},\n",
    "    parse_dates=['Date & Time'],\n",
    "    low_memory=False\n",
    ")\n",
    "print(\"Initial load - shape:\", df.shape)\n",
    "\n",
    "# Replace sentinel -1 with NA and convert to a nullable integer type\n",
    "df['Collision ID'] = df['Collision ID'].replace(-1, pd.NA).astype('Int32')\n",
    "\n",
    "# Example dropping or reassigning columns you won't use:\n",
    "# df.drop(columns=['Person ID', 'Latitude', 'Longitude'], errors='ignore', inplace=True)\n",
    "\n",
    "# 2) Fill missing for categorical columns\n",
    "categorical_cols = [\n",
    "    col for col in df.columns \n",
    "    if (df[col].dtype.name == 'category') or (df[col].dtype == object)\n",
    "]\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna('Unknown')  # assign back to df[col]\n",
    "\n",
    "# 3) Fill numeric columns with median\n",
    "numeric_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]\n",
    "for col in numeric_cols:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].median())  # assign back\n",
    "\n",
    "# 4) Convert boolean-like columns to 0/1 if they exist\n",
    "bool_cols = [\n",
    "    'Speed Related', 'Distracted Driver (Suspected)', \n",
    "    'Distracted Driver Related (Confirmed)', 'Suspected At Fault',\n",
    "    'Aggressive Driving Related', 'Red Light Running T/F',\n",
    "    'Hit & Run Related', 'Impaired Driving (Confirmed)'\n",
    "]\n",
    "for col in bool_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.upper()\n",
    "        df[col] = df[col].map({'Y':1, 'YES':1, 'T':1, 'N':0, 'NO':0, 'F':0})\n",
    "        df[col] = df[col].fillna(0).astype('int8')\n",
    "\n",
    "# 5) Handle high-cardinality columns\n",
    "remaining_cat_cols = []\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns and col not in bool_cols and col != 'Date & Time':\n",
    "        unique_count = df[col].nunique(dropna=True)\n",
    "        if unique_count > 100:\n",
    "            # We'll do label encoding\n",
    "            print(f\"[Label Encoding] '{col}' has {unique_count} unique categories.\")\n",
    "            encoder = LabelEncoder()\n",
    "            df[col] = encoder.fit_transform(df[col].astype(str))\n",
    "        else:\n",
    "            remaining_cat_cols.append(col)  # keep for one-hot if you want\n",
    "\n",
    "# 6) One-hot encode only the remaining moderate-cardinality columns\n",
    "if remaining_cat_cols:\n",
    "    print(\"One-hot encoding these columns:\", remaining_cat_cols)\n",
    "    df = pd.get_dummies(df, columns=remaining_cat_cols, drop_first=True)\n",
    "\n",
    "print(\"Shape after encoding:\", df.shape)\n",
    "\n",
    "# 7) Extract optional time-based features from 'Date & Time'\n",
    "if 'Date & Time' in df.columns:\n",
    "    df['Hour'] = df['Date & Time'].dt.hour\n",
    "    df['Weekday'] = df['Date & Time'].dt.dayofweek\n",
    "    df = df.drop(columns=['Date & Time'])\n",
    "\n",
    "print(\"Preprocessing complete. Final shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "924f03f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Label\n",
      "Low Risk       3491492\n",
      "Medium Risk     297800\n",
      "High Risk        14975\n",
      "Name: count, dtype: int64\n",
      "   Severity Score Risk Label  Risk Score (0–100)\n",
      "0             1.0   Low Risk                 0.0\n",
      "1             1.0   Low Risk                 0.0\n",
      "2             1.0   Low Risk                 0.0\n",
      "3             1.0   Low Risk                 0.0\n",
      "4             1.0   Low Risk                 0.0\n",
      "5             1.0   Low Risk                 0.0\n",
      "6             1.0   Low Risk                 0.0\n",
      "7             1.0   Low Risk                 0.0\n",
      "8             1.0   Low Risk                 0.0\n",
      "9             1.0   Low Risk                 0.0\n",
      "(3804267, 73)\n"
     ]
    }
   ],
   "source": [
    "# 3.1 If 'Severity Score' was dropped or changed, ensure it's still in df or re-merge it\n",
    "# For example, if you originally stored it in another df or dropped it, you can re-attach it here\n",
    "# df = df.merge(severity_df, on='Collision ID')   # only if needed\n",
    "\n",
    "# 3.2 Define thresholds for low, medium, high:\n",
    "def label_risk(sev):\n",
    "    if sev >= 8.0:        # e.g., 8–10 => High\n",
    "        return 'High Risk'\n",
    "    elif sev >= 4.0:      # 4–7.99 => Medium\n",
    "        return 'Medium Risk'\n",
    "    else:                 # 1–3.99 => Low\n",
    "        return 'Low Risk'\n",
    "\n",
    "df['Risk Label'] = df['Severity Score'].apply(label_risk)\n",
    "\n",
    "# 3.3 Create a normalized 0–100 risk score (optional)\n",
    "min_score = df['Severity Score'].min()\n",
    "max_score = df['Severity Score'].max()\n",
    "df['Risk Score (0–100)'] = ((df['Severity Score'] - min_score) / (max_score - min_score) * 100).round(1)\n",
    "\n",
    "# Inspect the distribution of classes\n",
    "print(df['Risk Label'].value_counts())\n",
    "print(df[['Severity Score','Risk Label','Risk Score (0–100)']].head(10))\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c96493",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRisk Label\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 4.2 Train/test split (stratify by Risk Label to preserve class proportions)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     10\u001b[0m     X, \n\u001b[0;32m     11\u001b[0m     y, \n\u001b[0;32m     12\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[0;32m     13\u001b[0m     stratify\u001b[38;5;241m=\u001b[39my, \n\u001b[0;32m     14\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 4.3 Train Logistic Regression as an interpretable baseline\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2805\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2801\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2803\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m-> 2805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2806\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   2807\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2808\u001b[0m     )\n\u001b[0;32m   2809\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2807\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2801\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2803\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[0;32m   2805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2806\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2807\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2808\u001b[0m     )\n\u001b[0;32m   2809\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:263\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only supported for dataframes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m     )\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# TODO: we should probably use _is_pandas_df_or_series(X) instead but this\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# would require updating some tests such as test_train_test_split_mock_pandas.\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_polars_df_or_series(X):\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _polars_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:44\u001b[0m, in \u001b[0;36m_pandas_indexing\u001b[1;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m     39\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(key)):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# using take() instead of iloc[] ensures the return value is a \"proper\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# copy that will not raise SettingWithCopyWarning\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\u001b[38;5;241m.\u001b[39mtake(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# check whether we should index with loc or iloc\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc \u001b[38;5;28;01mif\u001b[39;00m key_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mloc\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[0;32m   4134\u001b[0m     indices,\n\u001b[0;32m   4135\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[0;32m   4136\u001b[0m     verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4137\u001b[0m )\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    895\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[0;32m    896\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m    897\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    898\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    899\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[0;32m    690\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    691\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    692\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    693\u001b[0m             ),\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m   1308\u001b[0m     values, indexer, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m   1309\u001b[0m )\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m func(arr, indexer, out, fill_value)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\lopez\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:369\u001b[0m, in \u001b[0;36m_view_wrapper.<locals>.wrapper\u001b[1;34m(arr, indexer, out, fill_value)\u001b[0m\n\u001b[0;32m    366\u001b[0m         fill_value \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM8[ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    367\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m fill_wrap(fill_value)\n\u001b[1;32m--> 369\u001b[0m f(arr, indexer, out, fill_value\u001b[38;5;241m=\u001b[39mfill_value)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 4.1 Separate features (X) and the classification target (y)\n",
    "# drop the columns you don't want to feed into the model\n",
    "X = df.drop(columns=['Severity Score','Risk Score (0–100)','Risk Label', 'Work Zone Indicator']) \n",
    "y = df['Risk Label']\n",
    "\n",
    "# 4.2 Train/test split (stratify by Risk Label to preserve class proportions)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n",
    "\n",
    "# 4.3 Train Logistic Regression as an interpretable baseline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(\n",
    "    multi_class='ovr', \n",
    "    solver='saga',        # good for large datasets\n",
    "    penalty='l2', \n",
    "    C=1.0, \n",
    "    max_iter=1000, \n",
    "    n_jobs=-1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# 4.4 Evaluate the logistic model\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cd68517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Risk Label\n",
       "Low Risk       3491492\n",
       "Medium Risk     297800\n",
       "High Risk        14975\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62a586c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   High Risk       1.00      1.00      1.00      2995\n",
      "    Low Risk       1.00      1.00      1.00    698299\n",
      " Medium Risk       1.00      1.00      1.00     59560\n",
      "\n",
      "    accuracy                           1.00    760854\n",
      "   macro avg       1.00      1.00      1.00    760854\n",
      "weighted avg       1.00      1.00      1.00    760854\n",
      "\n",
      "{'High Risk': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2995.0}, 'Low Risk': {'precision': 1.0, 'recall': 0.999994271794747, 'f1-score': 0.9999971358891704, 'support': 698299.0}, 'Medium Risk': {'precision': 0.9999328453428246, 'recall': 1.0, 'f1-score': 0.9999664215439374, 'support': 59560.0}, 'accuracy': 0.9999947427495945, 'macro avg': {'precision': 0.9999776151142749, 'recall': 0.999998090598249, 'f1-score': 0.9999878524777026, 'support': 760854.0}, 'weighted avg': {'precision': 0.9999947431026434, 'recall': 0.9999947427495945, 'f1-score': 0.999994742830331, 'support': 760854.0}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(\n",
    "    class_weight='balanced_subsample',\n",
    "    n_estimators=100, \n",
    "    max_depth=None, \n",
    "    min_samples_leaf=3, \n",
    "    n_jobs=-1, \n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "report = classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd45a9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 RF Feature Importances:\n",
      "  # of Fatalities per Crash = 0.2068\n",
      "  KABCO Severity_(K) Fatal Injury = 0.2057\n",
      "  KABCO Severity_(O) No Injury = 0.1640\n",
      "  KABCO Severity_(B) Suspected Minor/Visible Injury = 0.1501\n",
      "  # of Injuries per crash  = 0.1184\n",
      "  KABCO Severity_(C) Possible Injury / Complaint = 0.0859\n",
      "  Driver Age (Crash Level)  = 0.0134\n",
      "  Driver Condition (Crash Level)  = 0.0109\n",
      "  Vehicle Contributing Factor (Crash Level) = 0.0054\n",
      "  Suspected At Fault.1 = 0.0051\n",
      "\n",
      "Logistic Regression Coefficients for 'High Risk':\n",
      "  Long: 0.0000\n",
      "  Posted Speed (Crash Level) : 0.0000\n",
      "  # of Fatalities per Crash: 0.0000\n",
      "  KABCO Severity_(K) Fatal Injury: 0.0000\n",
      "  # of Injuries per crash : 0.0000\n",
      "  Light Conditions (Crash Level)_Dark-Not Lighted: 0.0000\n",
      "  Drug Test Results (Crash Level) _Pending: 0.0000\n",
      "  Drug Test Results (Crash Level) _[\"Pending\",\"Pending\"]: 0.0000\n",
      "  Drug Test Results (Crash Level) _Marijuana: 0.0000\n",
      "  Weather Conditions (Crash Level)_Fog: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# 5.1 Random Forest Feature Importances\n",
    "importances = rf_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "top_n = 10  # top 10\n",
    "print(\"Top 10 RF Feature Importances:\")\n",
    "for idx in indices[:top_n]:\n",
    "    print(f\"  {feature_names[idx]} = {importances[idx]:.4f}\")\n",
    "\n",
    "# 5.2 Logistic Regression Coefficients (one-vs-rest)\n",
    "classes = log_reg.classes_  # e.g. ['High Risk','Low Risk','Medium Risk'] in alphabetical order\n",
    "print(\"\\nLogistic Regression Coefficients for 'High Risk':\")\n",
    "high_idx = list(classes).index('High Risk')\n",
    "coefs_high = log_reg.coef_[high_idx]\n",
    "coef_sorted_idx = np.argsort(coefs_high)[::-1]\n",
    "for idx in coef_sorted_idx[:top_n]:\n",
    "    print(f\"  {feature_names[idx]}: {coefs_high[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ec752aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predicted Risk Score Predicted Label Actual Label\n",
      "0                   0.1        Low Risk     Low Risk\n",
      "1                   0.3        Low Risk     Low Risk\n",
      "2                   0.0        Low Risk     Low Risk\n",
      "3                   0.1        Low Risk     Low Risk\n",
      "4                   0.3        Low Risk     Low Risk\n",
      "5                   0.0        Low Risk     Low Risk\n",
      "6                   0.5        Low Risk     Low Risk\n",
      "7                   0.0        Low Risk     Low Risk\n",
      "8                   1.6        Low Risk     Low Risk\n",
      "9                   0.4        Low Risk     Low Risk\n"
     ]
    }
   ],
   "source": [
    "# logistic probabilities on the test set\n",
    "probs_lr = log_reg.predict_proba(X_test)\n",
    "high_risk_index = list(log_reg.classes_).index('High Risk')\n",
    "high_risk_prob = probs_lr[:, high_risk_index]  # probability of High Risk\n",
    "\n",
    "# convert to 0-100\n",
    "pred_risk_score = (high_risk_prob * 100).round(1)\n",
    "\n",
    "# show a few examples\n",
    "results_df = pd.DataFrame({\n",
    "    \"Predicted Risk Score\": pred_risk_score[:10],\n",
    "    \"Predicted Label\": y_pred_lr[:10],\n",
    "    \"Actual Label\": y_test.iloc[:10].values\n",
    "})\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
